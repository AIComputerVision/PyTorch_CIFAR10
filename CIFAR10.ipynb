{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning PyTorch vision models to work with CIFAR-10 dataset\n",
    "### Author: Huy Phan\n",
    "### Github: https://github.com/huyvnphan/PyTorch-CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm_notebook as pbar\n",
    "from tensorboardX import SummaryWriter\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(params):\n",
    "    \"\"\"\n",
    "    Make a Pytorch dataloader object that can be used for traing and valiation\n",
    "    Input:\n",
    "        - params dict with key 'path' (string): path of the dataset folder\n",
    "        - params dict with key 'batch_size' (int): mini-batch size\n",
    "        - params dict with key 'num_workers' (int): number of workers for dataloader\n",
    "    Output:\n",
    "        - trainloader and testloader (pytorch dataloader object)\n",
    "    \"\"\"\n",
    "    transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "    \n",
    "    transform_validation = transforms.Compose([transforms.ToTensor(),\n",
    "                                               transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root=params['path'], train=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root=params['path'], train=False, transform=transform_validation)\n",
    "        \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=params['batch_size'], shuffle=True, num_workers=params['num_workers'])\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=params['batch_size'], shuffle=False, num_workers=params['num_workers'])\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params):\n",
    "    \n",
    "    writer = SummaryWriter('runs/' + params['description'])\n",
    "    model = model.to(params['device'])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], \n",
    "                          weight_decay=params['weight_decay'], momentum=0.9, nesterov=True)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=params['reduce_learning_rate'], gamma=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_accuracy = test_model(model, params)\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    for epoch in pbar(range(params['num_epochs'])):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            \n",
    "            # Loss accumulator for each epoch\n",
    "            logs = {'Loss': 0.0,\n",
    "                    'Accuracy': 0.0}\n",
    "            \n",
    "            # Set the model to the correct phase\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            \n",
    "            # Iterate over data\n",
    "            for image, label in params[phase+'_loader']:\n",
    "                image = image.to(params['device'])\n",
    "                label = label.to(params['device'])\n",
    "                \n",
    "                # Zero gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    prediction = model(image)\n",
    "                    loss = criterion(prediction, label)\n",
    "                    accuracy = torch.sum(torch.max(prediction, 1)[1] == label.data).item()\n",
    "\n",
    "                    # Update log\n",
    "                    logs['Loss'] += image.shape[0]*loss.detach().item()\n",
    "                    logs['Accuracy'] += accuracy\n",
    "\n",
    "                    # Backward pass\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            # Normalize and write the data to TensorBoard\n",
    "            logs['Loss'] /= len(params[phase+'_loader'].dataset)\n",
    "            logs['Accuracy'] /= len(params[phase+'_loader'].dataset)\n",
    "            writer.add_scalars('Loss', {phase: logs['Loss']}, epoch)\n",
    "            writer.add_scalars('Accuracy', {phase: logs['Accuracy']}, epoch)\n",
    "\n",
    "            # Save the best weights\n",
    "            if phase == 'validation' and logs['Accuracy'] > best_accuracy:\n",
    "                best_accuracy = logs['Accuracy']\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                 \n",
    "        # Write best weights to disk\n",
    "        if epoch % params['check_point'] == 0 or epoch == params['num_epochs']-1:\n",
    "            torch.save(best_model, params['state_dict_path'] + params['description'] + '.pt')\n",
    "    \n",
    "    final_accuracy = test_model(model, params)\n",
    "    writer.add_text('Final_Accuracy', str(final_accuracy), 0)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, params):\n",
    "    \n",
    "    model = model.to(params['device']).eval()     \n",
    "    phase = 'validation'\n",
    "    logs = {'Accuracy': 0.0}\n",
    "            \n",
    "    # Iterate over data\n",
    "    for image, label in pbar(params[phase+'_loader']):\n",
    "        image = image.to(params['device'])\n",
    "        label = label.to(params['device'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image)\n",
    "            accuracy = torch.sum(torch.max(prediction, 1)[1] == label.data).item()\n",
    "            logs['Accuracy'] += accuracy\n",
    "            \n",
    "    logs['Accuracy'] /= len(params[phase+'_loader'].dataset)\n",
    "    \n",
    "    return logs['Accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create PyTorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on cuda if available\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {'path': '/raid/data/pytorch_dataset/cifar10',\n",
    "               'batch_size': 256, 'num_workers': 4}\n",
    "\n",
    "train_loader, validation_loader = make_dataloaders(data_params)\n",
    "\n",
    "train_params = {'description': 'test',\n",
    "                'num_epochs': 600,\n",
    "                'reduce_learning_rate': [200, 400],\n",
    "                'learning_rate': 5e-2, 'weight_decay': 1e-3,\n",
    "                'check_point': 100, 'device': device,\n",
    "                'state_dict_path': 'trained_models/',\n",
    "                'train_loader': train_loader, 'validation_loader': validation_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(model, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, a_model in enumerate(all_models):\n",
    "#     train_params['description'] = descriptions[i]\n",
    "#     train_model(a_model, train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
